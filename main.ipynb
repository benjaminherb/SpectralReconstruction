{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "83baf3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Conv2D, MaxPool2D, Flatten, UpSampling3D, UpSampling2D, Conv3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "import matplotlib.pyplot as plt\n",
    "from HSI2RGB import HSI2RGB\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "280bbc20-a33b-4583-8917-0959d6150fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_16 (Conv2D)          (None, 64, 64, 31)        124       \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 64, 64, 31)        8680      \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 64, 64, 31)        8680      \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 64, 64, 31)        8680      \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 64, 64, 31)        8680      \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 64, 64, 31)        8680      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 43,524\n",
      "Trainable params: 43,524\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def _loss_function(ground_truth, prediction):\n",
    "    squared_difference = tf.square(ground_truth - prediction)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "model = Sequential([\n",
    "    Input([64,64,3]),\n",
    "    Conv2D(filters=31, kernel_size=(1,1), activation=\"relu\"),\n",
    "    Conv2D(filters=31, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n",
    "    Conv2D(filters=31, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n",
    "    Conv2D(filters=31, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),  \n",
    "    Conv2D(filters=31, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),\n",
    "    Conv2D(filters=31, kernel_size=(3,3), activation=\"relu\", padding=\"same\"),  \n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.002), loss=_loss_function, metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7558c371-7427-4846-9947-cb4a8c3dfc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, listdir\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "def load_images(load_dir):\n",
    "    rgb_paths = [path.join(load_dir, \"rgb\", image_path) for image_path in listdir(path.join(load_dir,\"rgb\"))]\n",
    "    hsi_paths = [path.join(load_dir, \"spectral\", image_path) for image_path in listdir(path.join(load_dir, \"spectral\"))]\n",
    "    \n",
    "    rgb_images = []\n",
    "    for rgb_path in rgb_paths:\n",
    "        rgb_images.append(cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB) / 255)\n",
    "    \n",
    "    hsi_images = []\n",
    "    for hsi_path in hsi_paths:\n",
    "        with h5py.File(hsi_path, 'r') as hf:\n",
    "            # swapping axis to match the hight x width x color form of the rgb images\n",
    "            hsi_images.append(np.array(hf['cube']).swapaxes(0,2))\n",
    "            bands = np.array(hf['bands'])\n",
    "\n",
    "    print(f\"Loaded {len(rgb_images)} RGB and {len(hsi_images)} HSI Images\")\n",
    "    return rgb_images, hsi_images, bands\n",
    "\n",
    "def load_hsi(hsi_path):\n",
    "    with h5py.File(hsi_path, 'r') as hf:\n",
    "        return np.array(hf['cube']).swapaxes(0,2)\n",
    "\n",
    "def split_images(rgb_images, hsi_images):\n",
    "    split_rgb = []\n",
    "    split_hsi = []\n",
    "    for rgb, hsi in zip(rgb_images, hsi_images):\n",
    "        height, width, _ = rgb.shape\n",
    "        # make sure each image overlaps with at least 8 pixels (48+2*8=64)\n",
    "        # on each side to avoid stitching problems\n",
    "        y_split = int(np.floor(height / 48)) + 1\n",
    "        x_split = int(np.floor(width / 48)) + 1\n",
    "        \n",
    "        # decrease the step width to space out the patches more evenly\n",
    "        y_step = int(48 + np.floor((height - y_split * 48) / y_split))\n",
    "        x_step = int(48 + np.floor((width - x_split * 48) / x_split))\n",
    "    \n",
    "        for y in range(y_split):\n",
    "            for x in range(x_split):\n",
    "                # the last patch always go to the edge -> can overlapp more than the other patches\n",
    "                if x == x_split - 1 and y == y_split - 1:\n",
    "                    split_rgb.append(rgb[height - 64:,width - 64:])\n",
    "                    split_hsi.append(hsi[height - 64:,width - 64:])   \n",
    "\n",
    "                elif x == x_split-1:\n",
    "                    split_rgb.append(rgb[y*y_step:y*y_step+64,width-64:])\n",
    "                    split_hsi.append(hsi[y*y_step:y*y_step+64,width-64:])\n",
    "\n",
    "                elif y == y_split-1:\n",
    "                    split_rgb.append(rgb[height-64:,x*x_step:x*x_step+64])\n",
    "                    split_hsi.append(hsi[height-64:,x*x_step:x*x_step+64])\n",
    "                else:\n",
    "                    split_rgb.append(rgb[y*y_step:y*y_step+64,x*x_step:x*x_step+64])\n",
    "                    split_hsi.append(hsi[y*y_step:y*y_step+64,x*x_step:x*x_step+64])\n",
    "                    \n",
    "    return split_rgb, split_hsi\n",
    "\n",
    "def split_image(image):\n",
    "    split = []\n",
    "    height, width, _ = image.shape\n",
    "    # make sure each image overlaps with at least 8 pixels (48+2*8=64)\n",
    "    # on each side to avoid stitching problems\n",
    "    y_split = int(np.floor(height / 48)) + 1\n",
    "    x_split = int(np.floor(width / 48)) + 1\n",
    "\n",
    "    # decrease the step width to space out the patches more evenly\n",
    "    y_step = int(48 + np.floor((height - y_split * 48) / y_split))\n",
    "    x_step = int(48 + np.floor((width - x_split * 48) / x_split))\n",
    "\n",
    "    for y in range(y_split):\n",
    "        for x in range(x_split):\n",
    "            # the last patch always go to the edge -> can overlapp more than the other patches\n",
    "            if x == x_split - 1 and y == y_split - 1:\n",
    "                split.append(image[height - 64:,width - 64:])\n",
    "            elif x == x_split-1:\n",
    "                split.append(image[y*y_step:y*y_step+64,width-64:])\n",
    "            elif y == y_split-1:\n",
    "                split.append(image[height-64:,x*x_step:x*x_step+64])\n",
    "            else:\n",
    "                split.append(image[y*y_step:y*y_step+64,x*x_step:x*x_step+64])\n",
    "\n",
    "    return split\n",
    "\n",
    "def merge_image(splits, size):\n",
    "    height, width, depth = size\n",
    "    \n",
    "    y_split = int(np.floor(height / 48)) + 1\n",
    "    x_split = int(np.floor(width / 48)) + 1\n",
    "\n",
    "    y_step = int(48 + np.floor((height - y_split * 48) / y_split))\n",
    "    x_step = int(48 + np.floor((width - x_split * 48) / x_split))\n",
    "    \n",
    "    image = np.empty((height, width, depth))\n",
    "    splits = iter(splits)\n",
    "    \n",
    "    for y in range(y_split):\n",
    "        for x in range(x_split):\n",
    "            # the last patch always go to the edge -> can overlapp more than the other patches\n",
    "            if x == x_split - 1 and y == y_split - 1:\n",
    "                image[height - 64:,width - 64:] = next(splits)\n",
    "                \n",
    "            elif x == x_split-1:\n",
    "                image[y*y_step:y*y_step+64,width-64:] = next(splits)\n",
    "            elif y == y_split-1:\n",
    "                image[height-64:,x*x_step:x*x_step+64] = next(splits)\n",
    "            else:\n",
    "                image[y*y_step:y*y_step+64,x*x_step:x*x_step+64] = next(splits)\n",
    "                \n",
    "    return image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ed1d57c1-ddbc-45d9-9958-91824c99c03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 RGB and 50 HSI Images\n",
      "(6050, 64, 64, 31)\n",
      "(6050, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "rgb, hsi, bands = load_images(\"./data/train\")\n",
    "split_rgb, split_hsi = split_images(rgb, hsi)\n",
    "rgb_stack = np.stack(split_rgb)\n",
    "hsi_stack = np.stack(split_hsi)\n",
    "print(hsi_stack.shape)\n",
    "print(rgb_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "992c3aeb-5050-463b-bc81-b64d8f024ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: saving model to checkpoints/checkpoint.01-0.2959.hdf5\n",
      "109/109 - 75s - loss: 0.0226 - accuracy: 0.4814 - val_loss: 0.0379 - val_accuracy: 0.2959 - 75s/epoch - 688ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: saving model to checkpoints/checkpoint.02-0.3418.hdf5\n",
      "109/109 - 79s - loss: 0.0225 - accuracy: 0.4718 - val_loss: 0.0408 - val_accuracy: 0.3418 - 79s/epoch - 726ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: saving model to checkpoints/checkpoint.03-0.3213.hdf5\n",
      "109/109 - 87s - loss: 0.0224 - accuracy: 0.5083 - val_loss: 0.0402 - val_accuracy: 0.3213 - 87s/epoch - 801ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: saving model to checkpoints/checkpoint.04-0.3079.hdf5\n",
      "109/109 - 71s - loss: 0.0221 - accuracy: 0.5130 - val_loss: 0.0422 - val_accuracy: 0.3079 - 71s/epoch - 648ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 6: saving model to checkpoints/checkpoint.06-0.3333.hdf5\n",
      "109/109 - 80s - loss: 0.0218 - accuracy: 0.5249 - val_loss: 0.0421 - val_accuracy: 0.3333 - 80s/epoch - 738ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: saving model to checkpoints/checkpoint.07-0.3164.hdf5\n",
      "109/109 - 75s - loss: 0.0216 - accuracy: 0.5299 - val_loss: 0.0416 - val_accuracy: 0.3164 - 75s/epoch - 688ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: saving model to checkpoints/checkpoint.08-0.3255.hdf5\n",
      "109/109 - 81s - loss: 0.0216 - accuracy: 0.5349 - val_loss: 0.0400 - val_accuracy: 0.3255 - 81s/epoch - 740ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: saving model to checkpoints/checkpoint.09-0.3445.hdf5\n",
      "109/109 - 85s - loss: 0.0216 - accuracy: 0.5241 - val_loss: 0.0477 - val_accuracy: 0.3445 - 85s/epoch - 783ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: saving model to checkpoints/checkpoint.10-0.3202.hdf5\n",
      "109/109 - 88s - loss: 0.0215 - accuracy: 0.5326 - val_loss: 0.0429 - val_accuracy: 0.3202 - 88s/epoch - 811ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61a6859420>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.isdir(\"./checkpoints\"):\n",
    "    mkdir(\"./checkpoints\")\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%M%d_%H%M%S\")\n",
    "checkpoint_filepath = \"checkpoints/checkpoint.{epoch:02d}-VLOSS_{val_loss:.4f}-VACC_{val_accuracy:.4f}.hdf5\"\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath,\n",
    "                                                               save_weights_only = True,\n",
    "                                                               monitor = \"val_accuracy\",\n",
    "                                                               mode = \"max\",\n",
    "                                                               save_best_only = False,\n",
    "                                                               verbose = 1)\n",
    "\n",
    "\n",
    "model.fit(x=rgb_stack, y=hsi_stack, validation_split=0.1, batch_size=50, epochs=10, shuffle=True, verbose=2,callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac015dd6-e193-4612-85b5-2a8ce83a63bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = cv2.cvtColor(cv2.imread(\"data/train/rgb/ARAD_1K_0901.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "example_image_hsi = load_hsi(\"data/train/spectral/ARAD_1K_0901.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45807751-5262-406b-91c3-856d0b9601d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    image_splits = split_image(image)\n",
    "    height, width, depth = image.shape\n",
    "    reconstructed_splits = []\n",
    "    for split in image_splits:\n",
    "        reconstructed_split = model.predict(split.reshape(1,64,64,3), verbose=False)\n",
    "        reconstructed_splits.append(reconstructed_split)\n",
    "    reconstructed = merge_image(reconstructed_splits, (height, width, 31))\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "test_reconstructed_image = process_image(example_image / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed58320-623d-4315-bf0d-ec55f95dd654",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axis = plt.subplots(1,3, figsize=(12,36)) \n",
    "\n",
    "axis[0].imshow(np.stack((test_reconstructed_image[:,:,25],\n",
    "                     test_reconstructed_image[:,:,15],\n",
    "                     test_reconstructed_image[:,:,5]),\n",
    "                    axis=-1))\n",
    "axis[1].imshow(np.stack((example_image_hsi[:,:,25],\n",
    "                     example_image_hsi[:,:,15],\n",
    "                     example_image_hsi[:,:,5]),\n",
    "                    axis=-1))\n",
    "axis[2].imshow(example_image)\n",
    "\n",
    "np.median(test_reconstructed_image[:,:,:])\n",
    "np.median(test_reconstructed_image[:,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f408e-1a34-4d0f-8de1-ab4fd1219887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast-spectral",
   "language": "python",
   "name": "fast-spectral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

